{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Warm-up"
      ],
      "metadata": {
        "id": "LgpHWkVFBZql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve(img: np.array, kernel: np.array) -> np.array:\n",
        "  # size of result img\n",
        "  row = x_1.shape[0] - f_1.shape[0] + 1\n",
        "  col = x_1.shape[1] - f_1.shape[1] + 1\n",
        "  k = kernel.shape[0] \n",
        "  # result img   \n",
        "  convolved_img = np.zeros(shape=(row, col)) \n",
        "  for i in range(row):\n",
        "      for j in range(col):\n",
        "          # create patch\n",
        "          mat = img[i:i+k, j:j+k]\n",
        "          convolved_img[i, j] = relu(np.sum(np.multiply(mat, kernel)) + bias)\n",
        "  return convolved_img\n",
        "\n",
        "def maxpooling(img):\n",
        "  # size of result img\n",
        "  img_pool_size = (int(img.shape[0]/pooling_size[0]), int(img.shape[1]/pooling_size[1]))\n",
        "  # result img   \n",
        "  pool_img = np.zeros(shape=img_pool_size)\n",
        "  for i in range(img_pool_size[0]):\n",
        "    for j in range(img_pool_size[1]):\n",
        "        # create patch\n",
        "        mat = img[i:i+pooling_size[0], j:j+pooling_size[1]]\n",
        "        # max pool\n",
        "        pool_img[i, j] = mat.max()\n",
        "    return pool_img\n",
        "\n",
        "convolved_img = convolve(img=np.array(x_1), kernel=f_1)\n",
        "print(\"convolved image is :\")\n",
        "print(convolved_img)\n",
        "maxpooled_img = maxpooling(convolved_img)\n",
        "print(\"max pooled image is :\")\n",
        "print(maxpooled_img)"
      ],
      "metadata": {
        "id": "z-3Gy0prBY-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1"
      ],
      "metadata": {
        "id": "xsTApNPd5oNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.losses import categorical_crossentropy\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "(Img, Lbl), (Img_pred, Lbl_true) = fashion_mnist.load_data()\n",
        "# Split data into different groups\n",
        "Img_train, Img_test, Lbl_train, Lbl_test = train_test_split(Img, Lbl, test_size=0.2, random_state=42)\n",
        "Img_train = Img_train.astype('float32')/255 ; Img_test = Img_test.astype('float32')/255\n",
        "# Adding one dimention for convolutional input layer and converting the training and testing labels into one-hot encoding vectors:\n",
        "X_train = np.expand_dims(Img_train, axis=-1) ; X_test = np.expand_dims(Img_test, axis=-1) ; X_pred = np.expand_dims(Img_pred, axis=-1)\n",
        "Y_train = np_utils.to_categorical(Lbl_train) ; Y_test = np_utils.to_categorical(Lbl_test) ; Y_true = np_utils.to_categorical(Lbl_true)\n",
        "\n",
        "def Plot (Model_history,Lbl_pred):\n",
        "  ACC = Model_history.history['accuracy']\n",
        "  ACC_val = Model_history.history['val_accuracy']\n",
        "  Loss = Model_history.history['loss']\n",
        "  Loss_val = Model_history.history['val_loss']\n",
        "  print('\\nMax accuracy in train data:', max(ACC), ', happened in epoch:', ACC.index(max(ACC))+1)\n",
        "  print('Max accuracy in test data:', max(ACC_val), ', happened in epoch:', ACC_val.index(max(ACC_val))+1)\n",
        "  print('Min loss in train data:', min(Loss), ', happened in epoch:', Loss.index(min(Loss))+1,)\n",
        "  print('Min loss in test data:', min(Loss_val), ', happened in epoch:', Loss_val.index(min(Loss_val))+1,'\\n')\n",
        "  fig = plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1,2,1) ; plt.plot(ACC) ; plt.plot(ACC_val) ; plt.xlabel('Epochs') ; plt.ylabel('Accuracy')\n",
        "  plt.legend(['Train', 'Test']) ; plt.title('Accracy')\n",
        "  plt.subplot(1,2,2) ; plt.plot(Loss) ; plt.plot(Loss_val) ; plt.xlabel('Epochs') ; plt.ylabel('Loss')\n",
        "  plt.legend(['Train', 'Test']) ; plt.title('Loss')\n",
        "  print(classification_report(Lbl_true, Lbl_pred, target_names=['T-shirt/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']),'\\n')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "AZuWh8uR0pDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "23B1FeIc-J6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Creation\n",
        "Model4Adam = Sequential()\n",
        "# Convolution and Maxpooling 1\n",
        "Model4Adam.add(Conv2D(64, (2, 2), strides=(1,1), padding='same', input_shape = X_train.shape[1:], name='conv1'))\n",
        "Model4Adam.add(Activation('softmax'))\n",
        "Model4Adam.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
        "Model4Adam.add(Dropout(.25))\n",
        "# Convolution and Maxpooling 2\n",
        "Model4Adam.add(Conv2D(64, (2, 2), strides=(1,1), padding='same', name='conv2'))\n",
        "Model4Adam.add(Activation('softmax'))\n",
        "Model4Adam.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))\n",
        "Model4Adam.add(Dropout(.25))\n",
        "# Convolution and Maxpooling 3\n",
        "Model4Adam.add(Conv2D(64, (2, 2), strides=(1,1), padding='same', name='conv3'))\n",
        "Model4Adam.add(Activation('softmax'))\n",
        "Model4Adam.add(MaxPooling2D(pool_size=(2, 2), name='pool3'))\n",
        "Model4Adam.add(Dropout(.25))\n",
        "# Convolution and Maxpooling 4\n",
        "Model4Adam.add(Conv2D(64, (2, 2), strides=(1,1), padding='same', name='conv4'))\n",
        "Model4Adam.add(Activation('softmax'))\n",
        "Model4Adam.add(MaxPooling2D(pool_size=(2, 2), name='pool4'))\n",
        "Model4Adam.add(Dropout(.25))\n",
        "# Converting to Fully Connected Input Shape\n",
        "Model4Adam.add(Flatten(name='flatten'))\n",
        "# FC layer\n",
        "Model4Adam.add(Dense(64, name='dense1'))\n",
        "Model4Adam.add(Activation('softmax'))\n",
        "Model4Adam.add(Dropout(.25))\n",
        "# Output layer\n",
        "Model4Adam.add(Dense(10, name='dense2'))\n",
        "Model4Adam.add(Activation('softmax'))\n",
        "Model4Adam.summary()\n",
        "\n",
        "Model4Adam.compile(optimizer=Adam(learning_rate=0.01), loss=categorical_crossentropy, metrics=['accuracy'])\n",
        "Model4Adam_history = Model4Adam.fit(X_train, Y_train, batch_size=128, epochs=100, validation_data=(X_test, Y_test))\n",
        "Y_pred = Model4Adam.predict(X_pred) ; Lbl_pred = np.argmax(Y_pred, axis=1)\n",
        "Plot(Model4Adam_history, Lbl_pred)"
      ],
      "metadata": {
        "id": "S4afQnN66XOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "Smw5TzEa-LQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "ACC_Relu = CFAR_model_history.history['acc']\n",
        "ACC_Relu_val = CFAR_model_history.history['val_acc']\n",
        "Loss_Relu =CFAR_model_history.history['loss']\n",
        "Loss_Relu_val=CFAR_model_history.history['val_loss']\n",
        "# Save model\n",
        "np.save('ACC_Relu_SGD', ACC_Relu)\n",
        "np.save('ACC_Relu_SGD_val', ACC_Relu_val)\n",
        "np.save('Loss_Relu_SGD', Loss_Relu)\n",
        "np.save('Loss_Relu_SGD_val', Loss_Relu_val)\n",
        "# # Load model\n",
        "# ACC_Relu = np.load('ACC_Relu_SGD.npy')\n",
        "# ACC_Relu_val=np.load('ACC_Relu_SGD_val.npy')\n",
        "# Loss_Relu = np.load('Loss_Relu_SGD.npy')\n",
        "# Loss_Relu_val=np.load('Loss_Relu_SGD_val.npy')"
      ],
      "metadata": {
        "id": "2lXzI70F3RzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2"
      ],
      "metadata": {
        "id": "hLMJyH9g-M68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_16 = cv2.normalize(x_test_16, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F) ; x_test_16 = x_test_16.astype(np.uint8)\n",
        "x_test_8 = cv2.normalize(x_test_8, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F) ; x_test_8 = x_test_8.astype(np.uint8)\n",
        "\n",
        "LR = 0.0001\n",
        "BATCH_SIZE=100\n",
        "input= tf.keras.layers.Input(shape =(32, 32, 3), batch_size=BATCH_SIZE)\n",
        "cnn1=tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=\"relu\", kernel_regularizer=regularizers.l2(LR), bias_regularizer=regularizers.l2(LR))(input)\n",
        "cnn1=tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=\"relu\", kernel_regularizer=regularizers.l2(LR), bias_regularizer=regularizers.l2(LR))(cnn1)\n",
        "cnn1= tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=\"relu\", kernel_regularizer=regularizers.l2(LR), bias_regularizer=regularizers.l2(LR))(cnn1)\n",
        "cnn1=tf.keras.layers.MaxPooling2D((2, 2), strides=2)(cnn1)\n",
        "cnn1=tf.keras.layers.Dropout(0.25)(cnn1)\n",
        "\n",
        "cnn2=tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=\"relu\", kernel_regularizer=regularizers.l2(LR), bias_regularizer=regularizers.l2(LR))(cnn1)\n",
        "cnn2=tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=\"relu\", kernel_regularizer=regularizers.l2(LR), bias_regularizer=regularizers.l2(LR))(cnn2)\n",
        "cnn2=tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=\"relu\", kernel_regularizer=regularizers.l2(LR), bias_regularizer=regularizers.l2(LR))(cnn2)\n",
        "cnn2=tf.keras.layers.MaxPooling2D((2, 2), strides=2)(cnn2)\n",
        "cnn2=tf.keras.layers.Dropout(0.25)(cnn2)\n",
        "\n",
        "flat=tf.keras.layers.Flatten()(cnn2)\n",
        "dense = tf.keras.layers.Dense(512, activation=\"relu\")(flat)\n",
        "dense=tf.keras.layers.Dropout(0.5)(dense)\n",
        "\n",
        "out =tf.keras.layers.Dense(10, activation=\"softmax\")(dense)\n",
        "model = tf.keras.Model(inputs = [input], outputs = [out])\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "x_train_32_32, x_val_32, y_train_32, y_val_32 = train_test_split(x_train_32_32, y_train, test_size=0.1, random_state=42)\n",
        "history = model.fit(x_train_32_32,y_train_32, epochs=500,validation_data=(x_val_32,y_val_32),batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "2we-TjAs-z60"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}