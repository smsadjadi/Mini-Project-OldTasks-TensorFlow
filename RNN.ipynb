{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIVTeljCgaooIhgTr0w/DT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hY0Lija_4jqr"},"outputs":[],"source":["import numpy as np\n","from pandas import DataFrame\n","from pandas import concat\n","import pandas\n","import time\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense,SimpleRNN,GRU\n","from keras import layers\n","from keras.models import Model\n","\n","# read and prepare data ################################################################################################################\n","df = pandas.read_csv('UberDemand.csv')\n","df1=np.asarray(df[['pickups','spd','vsb','temp','dewp','slp','pcp01','pcp06','pcp24','sd','hday','hr_sin','hr_cos','day_sin','day_cos']])\n","df2=np.asarray(df[['spd','vsb','temp','dewp','slp','pcp01','pcp06','pcp24','sd','hday','hr_sin','hr_cos','day_sin','day_cos']])\n","\n","d2=[(df2[4*i,]) for i in range(4343)]\n","d2=np.reshape(d2,[4343,14])\n","\n","a=np.zeros((4343, 4))\n","for i in range (4343):\n","   a[i,0] =df1[4*i,0]\n","   a[i,1] =df1[(4*i)+1,0]\n","   a[i,2] =df1[(4*i)+2,0]\n","   a[i,3] =df1[(4*i)+3,0]\n","mydata=np.concatenate((a,d2),1)\n","\n","# correlogram\n","import seaborn as sns\n","sns.pairplot(DataFrame(mydata))\n","plt.show()\n","plt.plot(df['hr_sin'],df['hr_cos'])"]},{"cell_type":"markdown","source":["Encoder"],"metadata":{"id":"1aAPrioJEi2a"}},{"cell_type":"code","source":["# Encoder Layer\n","encoding_dim = 1\n","x_train=mydata[:,9:12]\n","x_train = x_train.reshape(433,3)\n","encoding_dim = 1\n","input_img = layers.Input(shape=(3,))\n","encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n","decoded = layers.Dense(3, activation='sigmoid')(encoded)\n","autoencoder_Model = Model(input_img, decoded)\n","encoder_Model = Model(input_img, encoded)\n","encoded_input = layers.Input(shape=(encoding_dim,))\n","decoder_layer = autoencoder_Model.layers[-1]\n","decoder_Model = Model(encoded_input, decoder_layer(encoded_input))\n","autoencoder_Model.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","#  train our autoencoder\n","n_epochs = 1000\n","autoencoder_Model.fit(x_train, x_train, epochs=n_epochs, batch_size=256, validation_split=0.2, shuffle=True,)\n","pcp = encoder_Model.predict(x_train)\n","pcp /=max(pcp)\n","\n","mydata=np.delete(mydata,[11,9,10],1)   # dewp removing (dewp and temp have strong correlation)\n","mydata=np.concatenate((mydata,pcp),1)"],"metadata":{"id":"xjsJvwjX8xtB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare time and date\n","time_day=np.zeros((4343, 1))\n","for i in range (4343):\n","    time_day[i,0]=i%24\n","    #time_day[i,0]=(i//24)%7\n","mydata=np.delete(mydata,[6,8,9,10,12,14,15,16,17],1)  \n","mydata=np.concatenate((mydata,time_day),1)\n","\n","# normalize Data\n","for i in range (mydata.shape[1]): mydata[:,i] /=max(mydata[:,i])\n","\n","# create train and test data for last 6 hours\n","mydata = DataFrame(mydata)\n","data = list()\n","for i in range(7): data.append(mydata.shift(-6+i))\n","data = concat(data, axis=1)\n","data=data.values\n","\n","X_train = data[:3800, mydata.shape[1]:]\n","X_train = X_train.reshape((X_train.shape[0], 6, mydata.shape[1]))\n","Y_train = data[:3800, :4]\n","\n","X_test = data[3801:4300, mydata.shape[1]:]\n","X_test = X_test.reshape((X_test.shape[0], 6, mydata.shape[1]))\n","Y_test = data[3801:4300, :4]"],"metadata":{"id":"svOZgIF882JL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GRU Model"],"metadata":{"id":"mT7i07mc_Two"}},{"cell_type":"code","source":["# network parameter\n","epochs = 100\n","batch_size = 50\n","loss='mae'\n","optimizer='adam'\n","\n","# Network\n","model = Sequential()\n","model.add(GRU(40, input_shape=(X_train.shape[1], X_train.shape[2]), name=\"GRU_Layer\"))\n","model.add(Dense(4, name=\"FC_Layer\"))\n","model.summary()\n","model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n","\n","start = time.clock()\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2, shuffle=False)\n","end = time.clock()"],"metadata":{"id":"TIEcbOaz9HfN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM"],"metadata":{"id":"EcEjMcbjAWrp"}},{"cell_type":"code","source":["#network parameter\n","epochs = 100\n","batch_size = 50\n","loss='mae'\n","optimizer='adam'\n","\n","# Network\n","model = Sequential()\n","model.add(LSTM(40, input_shape=(X_train.shape[1], X_train.shape[2]),name=\"LSTM_Layer\"))\n","model.add(Dense(4, name=\"FC_Layer\"))\n","model.summary()\n","model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n","\n","start = time.clock()\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2, shuffle=False)\n","end = time.clock()"],"metadata":{"id":"6Gxm8Smy_cel"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SimpleRNN"],"metadata":{"id":"OmCcqpS5Ba5N"}},{"cell_type":"code","source":["#network parameter\n","epochs = 100\n","batch_size = 50\n","loss='mae'\n","optimizer='adam'\n","\n","# Network\n","model = Sequential()\n","model.add(SimpleRNN(40, input_shape=(X_train.shape[1], X_train.shape[2]), name=\"RNN_Layer\"))\n","model.add(Dense(4, name=\"FC_Layer\"))\n","model.summary()\n","model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n"," \n","start = time.clock()\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2, shuffle=False)\n","end = time.clock()"],"metadata":{"id":"YkEEOqPsBsaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting Results"],"metadata":{"id":"EeCbwH7fC9s5"}},{"cell_type":"code","source":["print( \"In this model \" + str(epochs) + \" epochs was trained in \"  + str((end - start)) + \" seconds.\"\n","      + \" Training loss is \" +str(history.history['loss'][-1]) + \"and validation loss is \" + str(history.history['val_loss'][-1])\n","      + \". Training accuracy is \" +str(history.history['acc'][-1]) + \"and validation accuracy is \" + str(history.history['val_acc'][-1]))\n","\n","# plot history\n","plt.figure()\n","plt.xlabel('Epochs number')\n","plt.ylabel('loss') \n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.legend(['train loss','validation loss'])\n","plt.show()\n","\n","plt.figure()\n","plt.xlabel('Epochs number')\n","plt.ylabel('acc') \n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.legend(['train acc','validation acc'])\n","plt.show()\n","\n","# Evaluate Model\n","test_loss, test_acc = model.evaluate(X_test,Y_test)\n","print( ' Test accuracy is ' + str(test_acc))\n","print( ' Test loss is ' +  str(test_loss))\n","\n","training_prediction = model.predict(X_train)\n","testing_prediction = model.predict(X_test)\n","\n","# For training data\n","tittle=[\"Bronx\",\"Brooklyn\",\"Manhattan\",\"Queens\"]\n","plt.figure(figsize=(10,6))\n","for i in range (4):\n","    plt.subplot(2, 2,i+1 )\n","    plt.tight_layout()\n","    plt.xlabel('Data nember')\n","    plt.ylabel('normalize pickups')\n","    plt.plot(Y_train[:,i],'r',label='train Data')\n","    plt.plot(training_prediction[:,i], 'b',label='predicted')\n","    plt.legend()\n","    plt.title(tittle[i])\n","\n","plt.figure(figsize=(10,6))\n","for i in range (4):\n","    plt.subplot(2, 2,i+1 )\n","    plt.tight_layout()\n","    plt.xlabel('Data nember')\n","    plt.ylabel('normalize pickups')\n","    plt.plot(Y_test[:,i],'r',label='test Data')\n","    plt.plot(testing_prediction[:,i], 'b',label='predicted')\n","    plt.legend()\n","    plt.title(tittle[i])"],"metadata":{"id":"vyjKWguuC__z"},"execution_count":null,"outputs":[]}]}