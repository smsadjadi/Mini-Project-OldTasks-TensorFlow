{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsPt10kEBOUi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import zlib\n",
        "import itertools\n",
        "import sklearn\n",
        "import itertools\n",
        "import scipy\n",
        "import skimage\n",
        "from skimage.transform import resize\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import keras \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import models, layers, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.models import Model\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn import preprocessing\n",
        "from keras.applications.nasnet import NASNetLarge, NASNetMobile\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "################                                           ####################\n",
        "################             Reading the Data              ####################\n",
        "################                                           ####################\n",
        "\n",
        "imageSize=224\n",
        "\n",
        "train_dir =  \"G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FULL FINAL DATASET/train/\"\n",
        "test_dir  =  \"G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FULL FINAL DATASET/test/\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_data(folder):\n",
        "    # Load the data and labels from the given folder.\n",
        "    X = [] ; y = []\n",
        "    for folderName in os.listdir(folder):\n",
        "        if not folderName.startswith('.'):\n",
        "            if (folderName in ['NORMAL']):\n",
        "                label = 0\n",
        "            elif folderName in ['BACTERIAL']:\n",
        "                label = 1\n",
        "            elif folderName in ['NON-COVID19 VIRAL']:\n",
        "                label = 2\n",
        "            elif folderName in ['COVID19 VIRAL']:\n",
        "                label = 3\n",
        "            else:\n",
        "                label = 4\n",
        "            for image_filename in tqdm(os.listdir(folder + folderName)):\n",
        "                img_file = cv2.imread(folder + folderName + '/' + image_filename)\n",
        "                if img_file is not None:\n",
        "                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n",
        "                    img_arr = np.asarray(img_file)\n",
        "                    X.append(img_arr)\n",
        "                    y.append(label)\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    return X,y\n",
        "\n",
        "X_train, y_train = get_data(train_dir) # Un-comment to use full dataset: Step 1 of 2\n",
        "X_test, y_test   = get_data(test_dir)\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=40) # Re-comment to use full dataset: Step 2 of 2\n",
        "\n",
        "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y_trainHot = to_categorical(y_train, num_classes = 4)\n",
        "y_testHot = to_categorical(y_test, num_classes = 4)\n",
        "\n",
        "print(len(y_trainHot)) ; print(len(y_testHot))\n",
        "\n",
        "def plotHistogram(a):\n",
        "    # Plot histogram of RGB Pixel Intensities\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(a)\n",
        "    plt.axis('off')\n",
        "    histo = plt.subplot(1,2,2)\n",
        "    histo.set_ylabel('Count')\n",
        "    histo.set_xlabel('Pixel Intensity')\n",
        "    n_bins = 30\n",
        "    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n",
        "    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n",
        "    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\n",
        "plotHistogram(X_train[1])\n",
        "\n",
        "print(\"Normal\")\n",
        "multipleImages = glob(\"G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FINAL AUGMENTED DATA/train/NORMAL/**\")\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:25]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) #.set_title(l)\n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1\n",
        "    \n",
        "print(\"Bacterial Infection\")\n",
        "multipleImages = glob(\"G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FINAL AUGMENTED DATA/train/BACTERIAL/**\")\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:25]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) #.set_title(l)\n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1\n",
        "        \n",
        "print(\"COVID19 Viral Infection\")\n",
        "multipleImages = glob(\"G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FINAL AUGMENTED DATA/train/COVID19 VIRAL/**\")\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:25]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) #.set_title(l)\n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1\n",
        "    \n",
        "print(\"NON-COVID19 Viral Infection\")\n",
        "multipleImages = glob(\"G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FINAL AUGMENTED DATA/train/NON-COVID19 VIRAL/**\")\n",
        "i_ = 0\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "for l in multipleImages[:25]:\n",
        "    im = cv2.imread(l)\n",
        "    im = cv2.resize(im, (128, 128)) \n",
        "    plt.subplot(5, 5, i_+1) #.set_title(l)\n",
        "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
        "    i_ += 1\n",
        "    \n",
        "map_characters = {0: 'Normal', 1: 'BACTERIAL', 2: 'NON-COVID19 VIRAL', 3: 'COVID19 VIRAL'}\n",
        "dict_characters=map_characters\n",
        "import seaborn as sns\n",
        "df = pd.DataFrame()\n",
        "df[\"labels\"]=y_train\n",
        "lab = df['labels']\n",
        "dist = lab.value_counts()\n",
        "sns.countplot(lab)\n",
        "print(dict_characters)\n",
        "\n",
        "# Helper Functions  Learning Curves and Confusion Matrix\n",
        "\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\" This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True` \"\"\"\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "map_characters1 = {0: 'Normal', 1: 'BACTERIAL', 2: 'NON-COVID19 VIRAL', 3: 'COVID19 VIRAL'}\n",
        "class_weight1 = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train) ## ? \n",
        "\n",
        "weight_path1 = 'imagenet'\n",
        "pretrained_model_1 = ResNet50(weights = weight_path1, include_top=False, input_shape=(imageSize, imageSize, 3))\n",
        "\n",
        "for i in range(np.shape(X_train)[0]):\n",
        "    for j in range(np.shape(X_train)[3]):        \n",
        "        X_train[i,:,:,j] = X_train[i,:,:,j] - np.min(X_train[i,:,:,j])\n",
        "        X_train[i,:,:,j] = X_train[i,:,:,j] / np.max(X_train[i,:,:,j])\n",
        "        X_train[i,:,:,j] = 2* X_train[i,:,:,j] - 1\n",
        "\n",
        "for i in range(np.shape(X_test)[0]):\n",
        "    for j in range(np.shape(X_test)[3]):\n",
        "        X_test[i,:,:,j] = (X_test[i,:,:,j] - np.min(X_test[i,:,:,j])) / np.max(X_test[i,:,:,j])\n",
        "        X_test[i,:,:,j] = X_test[i,:,:,j] / np.max(X_test[i,:,:,j])\n",
        "        X_test[i,:,:,j] = 2* X_test[i,:,:,j] - 1\n",
        "        \n",
        "# Deal with imbalanced class sizes below\n",
        "# Make Data 1D for compatability upsampling methods\n",
        "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
        "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
        "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
        "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n",
        "Y_train = y_train\n",
        "Y_test = y_test\n",
        "#ros = RandomOverSampler(sampling_strategy='auto')\n",
        "ros = RandomUnderSampler(sampling_strategy='auto')\n",
        "X_trainRos, Y_trainRos = ros.fit_sample(X_trainFlat, Y_train)\n",
        "#ros = ADASYN(sampling_strategy= 'not majority') \n",
        "ros = SMOTE(sampling_strategy='not majority', k_neighbors= 7)\n",
        "#ros = SVMSMOTE(sampling_strategy='not majority')\n",
        "X_testRos, Y_testRos = ros.fit_sample(X_testFlat, Y_test)\n",
        "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
        "Y_trainRosHot = to_categorical(Y_trainRos, num_classes = 4)\n",
        "Y_testRosHot = to_categorical(Y_testRos, num_classes = 4)\n",
        "# Make Data 2D again\n",
        "for i in range(len(X_trainRos)):\n",
        "    height, width, channels = imageSize,imageSize,3\n",
        "    X_trainRosReshaped = X_trainRos.reshape(len(X_trainRos),height,width,channels)\n",
        "for i in range(len(X_testRos)):\n",
        "    height, width, channels = imageSize,imageSize,3\n",
        "    X_testRosReshaped = X_testRos.reshape(len(X_testRos),height,width,channels)\n",
        "# Plot Label Distribution\n",
        "dfRos = pd.DataFrame()\n",
        "dfRos[\"labels\"]=Y_trainRos\n",
        "labRos = dfRos['labels']\n",
        "distRos = lab.value_counts()\n",
        "sns.countplot(labRos)\n",
        "print(dict_characters)\n",
        "\n",
        "class_weight1 = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
        "print(\"Old Class Weights: \",class_weight1)\n",
        "class_weight2 = class_weight.compute_class_weight('balanced', np.unique(Y_trainRos), Y_trainRos)\n",
        "print(\"New Class Weights: \",class_weight2)\n",
        "\n",
        "# Shuffle data to permit further subsampling\n",
        "from sklearn.utils import shuffle\n",
        "X_trainRosReshaped, Y_trainRosHot = shuffle(X_trainRosReshaped, Y_trainRosHot, random_state=13)\n",
        "X_testRosReshaped, Y_testRosHot = shuffle(X_testRosReshaped, Y_testRosHot, random_state=13)\n",
        "\n",
        "def get_model(pretrainedmodel,numclasses):\n",
        "#   base_model = pretrained_model_1 # Topless\n",
        "    base_model = pretrainedmodel # Topless\n",
        "    # Add top layer\n",
        "    x = base_model.output\n",
        "#   x = Conv2D(512, kernel_size=(3,3), padding = 'valid')(x)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "#   x = Flatten()(x)\n",
        "#   x = Dropout(0.2)(x)\n",
        "    predictions = Dense(numclasses, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    # Train top layer\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "# =============================================================================\n",
        "#     for layer in base_model.layers:\n",
        "#         if isinstance(layer, BatchNormalization):\n",
        "#             layer.trainable = True\n",
        "#         else:\n",
        "#             layer.trainable = False\n",
        "# =============================================================================\n",
        "    return model\n",
        "\n",
        "model = get_model(pretrained_model_1, 4)\n",
        "model.summary()\n",
        "\n",
        "optimizer1 = keras.optimizers.Adam(lr=1e-5)\n",
        "optimizer2 = keras.optimizers.RMSprop(lr=0.0001/2)\n",
        "optimizer3 = keras.optimizers.SGD(lr=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=optimizer1, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks_list = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
        "best_model     = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "set_session(sess)\n",
        "\n",
        "# Fit model\n",
        "xtrain = X_trainRosReshaped\n",
        "ytrain = Y_trainRosHot\n",
        "xtest = X_testRosReshaped\n",
        "ytest = Y_testRosHot\n",
        "numepochs = 50\n",
        "classweight = class_weight2 \n",
        "batch_size = 16\n",
        "\n",
        "###############################################################################\n",
        "##########                  Data Augmentation                        ##########\n",
        "###############################################################################\n",
        "\n",
        "# =============================================================================\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# \n",
        "# data_aug = ImageDataGenerator(rotation_range=5, \n",
        "#                               width_shift_range=0.1, \n",
        "#                               height_shift_range=0.1, \n",
        "#                               brightness_range=[0.8,1.2], \n",
        "#                               fill_mode='nearest')\n",
        "# \n",
        "# i = 0\n",
        "# \n",
        "# while i < 1:\n",
        "#     \n",
        "#     for X_batch in data_aug.flow_from_directory(\n",
        "#             directory = 'G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FULL FINAL DATASET/train/',\n",
        "#             classes   = ['NON-COVID19 VIRAL'],\n",
        "#             target_size=(496,768),\n",
        "#             batch_size= len(X_train),\n",
        "#             color_mode='rgb',\n",
        "#             class_mode=None,\n",
        "#             save_format='jpeg',\n",
        "#             save_to_dir='G:/DL-Based Detection of Covid-19 Subjects using Radiographic Images/FINAL AUGMENTED DATA/train/NON-COVID19 VIRAL/',\n",
        "#             save_prefix='aug'):\n",
        "#         \n",
        "#         break\n",
        "#     \n",
        "#     i += 1\n",
        "# =============================================================================\n",
        "\n",
        "###############################################################################\n",
        "##########                    Model Fitting                          ##########\n",
        "###############################################################################\n",
        "\n",
        "history = model.fit(xtrain,ytrain, epochs=numepochs, batch_size = batch_size, class_weight=classweight, validation_split = 0.2, verbose=1, callbacks = [callbacks_list, best_model])\n",
        "labels = map_characters1\n",
        "\n",
        "# Evaluate model\n",
        "score = model.evaluate(xtest,ytest, verbose=1)\n",
        "print('\\nKeras CNN - accuracy:', score[1], '\\n')\n",
        "y_pred = model.predict(xtest)\n",
        "print('\\n', sklearn.metrics.classification_report(np.where(ytest > 0)[1], np.argmax(y_pred, axis=1), target_names=list(labels.values())), sep='') \n",
        "Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
        "Y_true = np.argmax(ytest,axis = 1)\n",
        "y_true = Y_true\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "\n",
        "###############################################################################\n",
        "##########                           Plots                           ##########\n",
        "###############################################################################\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "# plt.savefig('G:/Prof. Soltanian-Zadeh, CT DATA/Accuracy_ResNet50.jpg', quality= 70, dpi= 500)\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "#plt.savefig('G:/Prof. Soltanian-Zadeh, CT DATA/Loss_ResNet50.jpg', quality= 70, dpi= 500)\n",
        "plt.show()\n",
        "\n",
        "plot_confusion_matrix(confusion_mtx, classes = list(labels.values()))\n",
        "#plt.savefig('G:/Prof. Soltanian-Zadeh, CT DATA/ConfMat_ResNet50.jpg', quality= 70, dpi= 500)\n",
        "\n",
        "###############################################################################\n",
        "##########               Class Activation Maps (CAMs)                ##########\n",
        "###############################################################################\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "img_path = ...\n",
        "\n",
        "preds     = np.zeros([50,4])\n",
        "preds_max = np.zeros(50)\n",
        "\n",
        "for count, img_filename in enumerate(os.listdir(img_path)):\n",
        "    img = image.load_img(img_path + '/' + img_filename, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = x - np.min(x)\n",
        "    x /= np.max(x)\n",
        "    x = x*2 - 1\n",
        "    preds[count,:]   = model.predict(x)[0]\n",
        "    preds_max[count] = np.argmax(preds) % 4\n",
        "    output = model.output[:, np.int(preds_max[count])]\n",
        "    last_conv_layer = model.get_layer('res5c_branch2c')\n",
        "    grads = K.gradients(output, last_conv_layer.output)[0]\n",
        "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "    pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "    for i in range(512):\n",
        "      conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "    heatmap -= np.mean(heatmap)\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap)\n",
        "    # heatmap = np.maximum(heatmap, 0)\n",
        "    # heatmap /= np.max(heatmap)\n",
        "\n",
        "    # plt.matshow(heatmap)\n",
        "    # plt.show()\n",
        "    \n",
        "    img = cv2.imread(img_path + '/' + img_filename)\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    hif = .6\n",
        "    superimposed_img = heatmap * hif + img\n",
        "    output = ...\n",
        "    cv2.imwrite(output %(count), superimposed_img)\n",
        "    # img=mpimg.imread(...)\n",
        "    # plt.imshow(img)\n",
        "    print(count)"
      ]
    }
  ]
}